import datetime
import pandas as pd

from typing import Tuple, Optional

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.python import PythonSensor
from airflow.providers.oracle.hooks.oracle import OracleHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable
from airflow.exceptions import AirflowFailException


default_args = {
    "owner": "Yestay",
    "email_on_failure": False,   
    "depends_on_past": False,
}

ORACLE_CONN_ID = "db_oracle_ipc__edw_ipc"
POSTGRES_CONN_ID = "db_postgres_monitoring__wf_monitor"
SENT_VAR_PREFIX = "pfm_acc_dep_openway_monitoring"

NEED = [
    {"workflow_id": 8661, "task_id": 8663},  # ACCOUNT
    {"workflow_id": 8671, "task_id": 8679},  # DEPOSIT
    {"workflow_id": 8690, "task_id": 8696},  # OPENWAY
]

SQL_MONITOR_WF_TASK_BY_ID = """
WITH need AS (
    SELECT 8661 AS workflow_id, 8663 AS task_id FROM dual UNION ALL
    SELECT 8671, 8679 FROM dual UNION ALL
    SELECT 8690, 8696 FROM dual
),
last_wf_run AS (
    SELECT
        r.workflow_id,
        r.workflow_name,
        r.workflow_run_id,
        r.start_time AS wf_start_time,
        r.end_time   AS wf_end_time,
        r.run_err_code,
        ROW_NUMBER() OVER (PARTITION BY r.workflow_id ORDER BY r.start_time DESC) rn
    FROM RB_REP.OPB_WFLOW_RUN r
    WHERE TRUNC(r.start_time) = TRUNC(SYSDATE)
      AND r.workflow_id IN (SELECT workflow_id FROM need)
),
ins_rows AS (
    SELECT
        s.workflow_run_id,
        MAX(s.affected_rows) AS inserted_rows
    FROM RB_REP.OPB_SWIDGINST_LOG s
    GROUP BY s.workflow_run_id
),
last_task_run AS (
    SELECT
        tr.workflow_run_id,
        tr.instance_id,
        tr.instance_name,
        tr.task_id,
        tr.start_time AS task_start_time,
        CASE
            WHEN tr.end_time IS NULL THEN NULL
            WHEN tr.start_time IS NOT NULL AND tr.end_time < tr.start_time THEN NULL
            WHEN tr.end_time > SYSDATE THEN NULL
            ELSE tr.end_time
        END AS task_end_time,
        tr.run_err_code AS task_run_err_code,
        ROW_NUMBER() OVER (
            PARTITION BY tr.workflow_run_id, tr.instance_id
            ORDER BY tr.start_time DESC
        ) rn
    FROM RB_REP.OPB_TASK_INST_RUN tr
    WHERE TRUNC(tr.start_time) = TRUNC(SYSDATE)
      AND tr.task_id IN (SELECT task_id FROM need)
)
SELECT
    w.workflow_name,
    w.workflow_id,
    w.workflow_run_id,
    w.wf_start_time,
    w.wf_end_time,
    CASE
        WHEN w.workflow_run_id IS NULL THEN 'NOT STARTED'
        WHEN w.wf_end_time IS NULL THEN 'RUNNING'
        WHEN NVL(w.run_err_code,0) <> 0 THEN 'ERROR/WARNING'
        ELSE 'SUCCESS'
    END AS wf_status,
    NVL(ir.inserted_rows, 0) AS inserted_rows,
    n.task_id,
    tr.instance_name,
    tr.task_start_time,
    tr.task_end_time,
    CASE
        WHEN w.workflow_run_id IS NULL THEN 'NO RUN'
        WHEN tr.task_start_time IS NULL THEN 'TASK NO RUN'
        WHEN tr.task_end_time IS NULL THEN 'TASK RUNNING'
        WHEN NVL(tr.task_run_err_code,0) <> 0 THEN 'TASK ERROR/WARNING'
        ELSE 'TASK SUCCESS'
    END AS task_status
FROM need n
LEFT JOIN last_wf_run w
    ON w.workflow_id = n.workflow_id
   AND w.rn = 1
LEFT JOIN ins_rows ir
    ON ir.workflow_run_id = w.workflow_run_id
LEFT JOIN last_task_run tr
    ON tr.workflow_run_id = w.workflow_run_id
   AND tr.task_id = n.task_id
   AND tr.rn = 1
ORDER BY n.workflow_id
"""


def _sent_key(workflow_id: int) -> str:
    return "{0}_{1}".format(SENT_VAR_PREFIX, int(workflow_id))


def _fetch_df() -> pd.DataFrame:
    oracle_hook = OracleHook(oracle_conn_id=ORACLE_CONN_ID, thick_mode=True)
    conn = oracle_hook.get_conn()
    try:
        with conn.cursor() as cursor:
            cursor.execute(SQL_MONITOR_WF_TASK_BY_ID)
            rows = cursor.fetchall()
            cols = [c[0] for c in cursor.description]
    finally:
        conn.close()
    return pd.DataFrame(rows, columns=cols)


def _row_for_workflow(df: pd.DataFrame, workflow_id: int) -> pd.DataFrame:
    if df.empty:
        return df
    return df[df["WORKFLOW_ID"] == workflow_id].copy()


def _is_ready(r: pd.Series) -> bool:
    return not (
        pd.isna(r["WORKFLOW_RUN_ID"])
        or pd.isna(r["WF_END_TIME"])
        or pd.isna(r["TASK_END_TIME"])
    )


def _final_status(r: pd.Series) -> str:
    wf_status = str(r.get("WF_STATUS") or "")
    task_status = str(r.get("TASK_STATUS") or "")

    if wf_status in ("NOT STARTED", "NO RUN"):
        return "NOT_STARTED"

    if wf_status == "RUNNING" or task_status in ("TASK RUNNING", "TASK NO RUN"):
        return "RUNNING"

    has_errors = (wf_status == "ERROR/WARNING") or (task_status == "TASK ERROR/WARNING")
    return "FAILED" if has_errors else "SUCCESS"


def _error_code_and_msg(r: pd.Series) -> Tuple[int, Optional[str]]:
    status = _final_status(r)
    if status == "SUCCESS":
        return 0, None
    if status == "FAILED":
        return 1, "WF_STATUS={0}; TASK_STATUS={1}".format(r.get("WF_STATUS"), r.get("TASK_STATUS"))
    if status == "RUNNING":
        return 2, "RUNNING"
    return 3, "NOT_STARTED"



def wait_one_done(workflow_id: int, task_id: int, **context) -> bool:
    today = datetime.date.today().isoformat()
    sent_var = _sent_key(workflow_id)
    last_sent = Variable.get(sent_var, default_var="1970-01-01")

    print("[INFO] wait_one_done wf={0} task={1} today={2} last_sent={3}".format(
        workflow_id, task_id, today, last_sent
    ))

    # если уже писали сегодня — считаем выполненным
    if last_sent == today:
        print("[INFO] Already logged today for this workflow. Sensor succeeds.")
        return True

    df = _fetch_df()
    if df.empty:
        print("[INFO] No rows yet. Waiting...")
        return False

    one = _row_for_workflow(df, workflow_id)
    if one.empty:
        print("[INFO] No row for this workflow yet. Waiting...")
        return False

    r = one.iloc[0]
    if not _is_ready(r):
        print("[INFO] Not finished yet. Waiting...")
        return False

    print("[INFO] Finished. Sensor succeeds.")
    return True



def write_one_log(workflow_id: int, task_id: int, **context) -> None:
    today_iso = datetime.date.today().isoformat()
    sent_var = _sent_key(workflow_id)
    last_sent = Variable.get(sent_var, default_var="1970-01-01")

    print("[INFO] write_one_log wf={0} today={1} last_sent={2}".format(workflow_id, today_iso, last_sent))

    if last_sent == today_iso:
        print("[INFO] Already logged today for this workflow. Skipping.")
        return

    df = _fetch_df()
    one = _row_for_workflow(df, workflow_id)
    if one.empty:
        raise AirflowFailException("No row for workflow_id={0}".format(workflow_id))

    r = one.iloc[0]
    if not _is_ready(r):
        raise AirflowFailException("Workflow not finished yet (workflow_id={0})".format(workflow_id))

    workflow_name = r.get("WORKFLOW_NAME")
    start_time = r.get("WF_START_TIME")
    end_time = r.get("WF_END_TIME")
    inserted_rows = int(r.get("INSERTED_ROWS") or 0)

    duration_min = None
    try:
        if start_time is not None and end_time is not None:
            duration_min = int((end_time - start_time).total_seconds() / 60)
    except Exception:
        duration_min = None

    status = _final_status(r)
    error_code, error_msg = _error_code_and_msg(r)

    mapping_name = r.get("INSTANCE_NAME")

    pg = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

    pg.run(
        """
        INSERT INTO public.pfm_etl_log (
            workflow_id,
            workflow_name,
            taks_name,
            start_time,
            end_time,
            duration_min,
            status,
            source_count,
            source_failed_rows,
            target_inserted,
            target_rejected_rows,
            error_code,
            error_msg,
            created_at
        )
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s, now())
        ON CONFLICT (workflow_id, start_time)
        DO UPDATE SET
            workflow_name         = EXCLUDED.workflow_name,
            taks_name             = EXCLUDED.taks_name,
            end_time              = EXCLUDED.end_time,
            duration_min          = EXCLUDED.duration_min,
            status                = EXCLUDED.status,
            source_count          = EXCLUDED.source_count,
            source_failed_rows    = EXCLUDED.source_failed_rows,
            target_inserted       = EXCLUDED.target_inserted,
            target_rejected_rows  = EXCLUDED.target_rejected_rows,
            error_code            = EXCLUDED.error_code,
            error_msg             = EXCLUDED.error_msg
        """,
        parameters=(
            int(workflow_id),
            workflow_name,
            taks_name,
            start_time,
            end_time,
            duration_min,
            status,
            inserted_rows,  # source_count
            0,              # source_failed_rows
            inserted_rows,  # target_inserted
            0,              # target_rejected_rows
            int(error_code),
            error_msg,
        ),
    )

    Variable.set(sent_var, today_iso)
    print("[INFO] Log upserted into pfm_etl_log. Variable {0} set to {1}".format(sent_var, today_iso))


with DAG(
    dag_id="pfm_acc_dep_openway_to_monitoring",
    default_args=default_args,
    start_date=datetime.datetime(2026, 1, 22),
    schedule="0 9 * * *",
    catchup=False,
    tags=["pfm", "monitoring"],
    max_active_runs=1,
) as dag:

    for item in NEED:
        wf_id = item["workflow_id"]
        task_id = item["task_id"]

        wait = PythonSensor(
            task_id="wait_wf_{0}_done".format(wf_id),
            python_callable=wait_one_done,
            op_kwargs={"workflow_id": wf_id, "task_id": task_id},
            mode="reschedule",
            poke_interval=600,        # 10 минут
            timeout=16 * 60 * 60,     # 16 часов
        )

        write_log = PythonOperator(
            task_id="log_wf_{0}_to_pg".format(wf_id),
            python_callable=write_one_log,
            op_kwargs={"workflow_id": wf_id, "task_id": task_id},
        )

        wait >> write_log

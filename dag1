from datetime import datetime, timedelta, date

from airflow import DAG
from airflow.sensors.base import BaseSensorOperator
from airflow.operators.python import PythonOperator
from airflow.providers.oracle.hooks.oracle import OracleHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.exceptions import AirflowFailException
from airflow.operators.dummy import DummyOperator
from typing import Dict, Any
from airflow.utils.dates import days_ago
from typing import Dict, List, Any
import pendulum

ORACLE_CONN_ID_IPC = "db_oracle_ipc__edw_ipc"
ORACLE_CONN_ID_EDW = 'EDW_ETL_CDO'
POSTGRES_CONN_ID = "db_postgres_monitoring__wf_monitor"

PROCESSING_DATE = "TRUNC(SYSDATE)"
# PROCESSING_DATE = "DATE'2025-12-31'"
#
WORKFLOW_CONFIGS = [
    {
        'workflow_id': 8661,  # wf_PROD_edw_to_operations_account
        'instance_id': 4659,  # m_PROD_account_edw_to_operations
        'name': 'ACCOUNT'
    },
    {
        'workflow_id': 8671,  # wf_PROD_edw_to_operations_deposit
        'instance_id': 4673,  # m_PROD_deposit_edw_to_operations
        'name': 'DEPOSIT'
    },
    {
        'workflow_id': 8690,  # wf_PROD_edw_to_operations_openway
        'instance_id': 4685,  # m_PROD_openway_edw_to_operations
        'name': 'OPENWAY'
    }
]

default_args = {
    'owner': 'Almas',
    'depends_on_past': False,
    'start_date': pendulum.datetime(2025, 12, 23, tz="UTC"),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


def get_processing_date():
    if PROCESSING_DATE == "TRUNC(SYSDATE-1)":
        return date.today() - timedelta(days=1)
    elif PROCESSING_DATE == "TRUNC(SYSDATE)":
        return date.today()
    elif PROCESSING_DATE.startswith("DATE'"):
        return datetime.strptime(
            PROCESSING_DATE.replace("DATE'", "").replace("'", ""),
            "%Y-%m-%d"
        ).date()
    else:
        return datetime.strptime(PROCESSING_DATE, "%Y-%m-%d").date()

PROCESSING_DATE = get_processing_date()

# -------------------------------------------------------------------
# SENSOR
# -------------------------------------------------------------------
class SourceWorkflowSensor(BaseSensorOperator):

    def __init__(
            self,
            oracle_conn_id: str,
            workflow_id: int,
            instance_id: int,
            processing_date: date,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.oracle_conn_id = oracle_conn_id
        self.workflow_id = workflow_id
        self.instance_id = instance_id
        self.processing_date = processing_date

    def poke(self, context: Dict[str, Any]) -> bool:
        hook = OracleHook(oracle_conn_id=self.oracle_conn_id, thick_mode=True)
        #conn = hook.get_conn()
        #cursor = conn.cursor()

        sql = """
        select 1
        from dual
        where exists (
            select 1
            from RB_REP.OPB_WFLOW_RUN wr
            join RB_REP.OPB_SESS_TASK_LOG sl
              on sl.workflow_id = wr.workflow_id
             and sl.workflow_run_id = wr.workflow_run_id
            join RB_REP.OPB_TASK_INST ti
              on ti.workflow_id = wr.workflow_id
             and ti.instance_id = sl.instance_id
            where trunc(wr.start_time) = :proc_date
              and wr.workflow_id = :workflow_id
              and ti.instance_id = :instance_id
              and ti.task_type = 68
        )
        """

        try:
            result = hook.get_first(
                sql,
                parameters={
                    "proc_date": self.processing_date, #datetime.strptime(self.processing_date,"%Y-%m-%d").date(),
                    "workflow_id": self.workflow_id,
                    "instance_id": self.instance_id,
                }
            )
            #print(result)
            flag = result is not None
            self.log.info(
                "workflow_id=%s instance_id=%s flag=%s",
                self.workflow_id, self.instance_id, flag
            )
            return flag

        except Exception as e:
            raise AirflowFailException(f"Source check failed: {str(e)}")


# -------------------------------------------------------------------
# LOAD TO POSTGRES
# -------------------------------------------------------------------

def load_to_pg(wf: dict):
    ora = OracleHook(oracle_conn_id=ORACLE_CONN_ID_IPC, thick_mode=True)
    pg = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

    sql = f"""
    select
        wr.workflow_id,
        trunc(sysdate-1) as date_value,
        wr.workflow_name,
        sl.mapping_name,
        wr.start_time,
        wr.end_time,
        round(to_number( (wr.end_time)-(wr.start_time)  )*24*60,2) as duration_min,
        CASE 
                    WHEN (sl.first_error_code = '0' and sl.src_success_rows-sl.targ_success_rows=0) THEN 'SUCCESS' else 'FAILED' 
                END as status,
        sl.src_success_rows as source_count,
        sl.src_failed_rows as source_failed_rows,
        sl.targ_success_rows as target_inserted,
        sl.targ_failed_rows as target_rejected_rows,
        sl.first_error_code as error_code,
        sl.first_error_msg as error_msg
    from RB_REP.OPB_WFLOW_RUN wr
    join RB_REP.OPB_SESS_TASK_LOG sl
      on sl.workflow_id = wr.workflow_id
     and sl.workflow_run_id = wr.workflow_run_id
    join RB_REP.OPB_TASK_INST ti
      on ti.workflow_id = wr.workflow_id
     and ti.instance_id = sl.instance_id
    where --trunc(wr.start_time) = :proc_date
      trunc(wr.start_time)>=date'2025-12-20' and trunc(wr.start_time)<date'2025-01-17'
      and wr.workflow_id = :wf_id
      and ti.instance_id = :ins_id
      and ti.task_type = 68
    """


    rows = ora.get_records(
        sql,
        parameters={
            "proc_date": PROCESSING_DATE, #datetime.strptime(processing_date,"%Y-%m-%d").date(),
            "wf_id": wf["workflow_id"],
            "ins_id": wf["instance_id"]
        }
    )
    print(rows)

    if not rows:
        raise ValueError("Нет данных для логирования")

    for r in rows:
        (   
            workflow_id,
            date_value,
            workflow_name,
            mapping_name,
            start_time,
            end_time,
            duration_min,
            status,
            source_count,
            source_failed_rows,
            target_inserted,
            target_rejected_rows,
            error_code,
            error_msg
        ) = r

        pg.run(
            """
            INSERT INTO public.pfm_etl_log (
                workflow_id,
                date_value,
                workflow_name,
                mapping_name,
                start_time,
                end_time,
                duration_min,
                status,
                source_count,
                source_failed_rows,
                target_inserted,
                target_rejected_rows,
                error_code,
                error_msg
            )
            VALUES (%s,%s,%s,%s,%s, %s,%s,%s,%s,%s,%s,%s,%s,%s)
            """,
            parameters=(
                workflow_id,
                date_value,
                workflow_name,
                mapping_name,
                start_time,
                end_time,
                duration_min,
                status,
                source_count,
                source_failed_rows,
                target_inserted,
                target_rejected_rows,
                error_code,
                error_msg,
            ),
        )


# -------------------------------------------------------------------
# DAG
# -------------------------------------------------------------------
with DAG(
        dag_id='PFM_monitoring',
        default_args=default_args,
        description='Мониторинг PFM за который просто спасибо сказали',
        schedule='0 4 * * *',  # every day at 9 o'clock
        catchup=False,
        max_active_runs=1,
        tags=['Almas A', 'PFM', 'monitoring', 'oracle to postgres']
) as dag:
    start_task = DummyOperator(task_id='start')
    end_task = DummyOperator(task_id='end')

    transfer_tasks = []

    for config in WORKFLOW_CONFIGS:
        name = config['name']

        # check flag task
        wait_sensor = SourceWorkflowSensor(
            task_id=f'check_{name}_ready',
            oracle_conn_id=ORACLE_CONN_ID_IPC,
            workflow_id=config["workflow_id"],
            instance_id=config['instance_id'],
            processing_date=PROCESSING_DATE,
            poke_interval=900,
            timeout=6 * 3600,
            mode='reschedule'
        )

        # transfer task
        transfer_task = PythonOperator(
            task_id=f"load_pg_{config['workflow_id']}",
            python_callable=load_to_pg,
            op_kwargs={"wf": config},
        )

        start_task >> wait_sensor >> transfer_task
        transfer_tasks.append(transfer_task)

    # connect all transfer tasks with the final
    if transfer_tasks:
        for transfer_task in transfer_tasks:
            transfer_task >> end_task
